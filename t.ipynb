{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gl/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision\n",
    "from typing import Optional, Union\n",
    "import pickle\n",
    "import model\n",
    "import configs\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision\n",
    "from typing import Optional, Union\n",
    "import pickle\n",
    "import model\n",
    "import configs\n",
    "import numpy as np\n",
    "import pickle\n",
    "import configs\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision\n",
    "from typing import Optional, Union\n",
    "import configs\n",
    "import pickle\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(52, 100, 25, 1).to(device)\n",
    "with open ('small_sample', 'rb') as fp:\n",
    "    data_feature = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'configs' has no attribute 'LSTM_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM_epoch\u001b[49m\n\u001b[1;32m      2\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39mLSTM_learning_rate\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'configs' has no attribute 'LSTM_epoch'"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate= 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = configs.training_size_scale\n",
    "\n",
    "\n",
    "    \n",
    "with open ('feature', 'rb') as fp:\n",
    "    data_feature = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open ('target', 'rb') as fp:\n",
    "    data_target = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "cap = int(len(data_feature)*train_size - 10000*train_size)\n",
    "train_set = data_feature[:cap]\n",
    "train_target = data_target[:cap]\n",
    "valid_set = data_feature[-15000:-10000]\n",
    "valid_target = data_target[-15000:-10000]\n",
    "test_set = data_feature[-10000:]\n",
    "test_target = data_target[-10000:]\n",
    "\n",
    "train_set = torch.Tensor(train_set).to(device)\n",
    "train_target = torch.Tensor(train_target).to(device)\n",
    "valid_set = torch.Tensor(valid_set).to(device)\n",
    "valid_target = torch.Tensor(valid_target).to(device)\n",
    "test_set = torch.Tensor(test_set).to(device)\n",
    "test_target = torch.Tensor(test_target).to(device)\n",
    "\n",
    "model = LSTMClassifier(52, 100, 25, 1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 64\n",
    "def get_batches(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in get_batches(train_set, train_target, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())  # Ensure labels are long type for CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    pred = model(valid_set)\n",
    "    vloss =  criterion(pred, valid_target)\n",
    "\n",
    "    pred_index = torch.max(pred, 1)[1]\n",
    "    correct = 0\n",
    "    for j in range (len(valid_target)):\n",
    "        if valid_target[j][pred_index[j]] == 1:\n",
    "            correct +=1\n",
    "    v_acc = correct/len(valid_target)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Finish epoch {}, loss: {}, vloss: {}, vAcc: {}\".format(i, running_loss/len(train_set), vloss, v_acc))\n",
    "\n",
    "model.eval()\n",
    "pred = model(test_set)\n",
    "pred_index = torch.max(pred, 1)[1]\n",
    "correct = 0\n",
    "for i in range (len(test_target)):\n",
    "    if test_target[i][pred_index[i]] == 1:\n",
    "        correct +=1\n",
    "\n",
    "print(\"accruracy: \", correct/len(test_target))\n",
    "\n",
    "#torch.save(model.state_dict(), \"./models/cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = configs.cnn_epoch\n",
    "learning_rate= configs.cnn_learning_rate\n",
    "train_size = configs.training_size_scale\n",
    "\n",
    "with open ('feature', 'rb') as fp:\n",
    "    data_feature = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open ('target', 'rb') as fp:\n",
    "    data_target = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.1\n",
    "cap = int(len(data_feature)*train_size - 10000*train_size)\n",
    "train_set = data_feature[:cap]\n",
    "train_target = data_target[:cap]\n",
    "valid_set = data_feature[-15000:-10000]\n",
    "valid_target = data_target[-15000:-10000]\n",
    "test_set = data_feature[-10000:]\n",
    "test_target = data_target[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.Tensor(train_set).to(device)\n",
    "train_target = torch.Tensor(train_target).to(device)\n",
    "valid_set = torch.Tensor(valid_set).to(device)\n",
    "valid_target = torch.Tensor(valid_target).to(device)\n",
    "test_set = torch.Tensor(test_set).to(device)\n",
    "test_target = torch.Tensor(test_target).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(52, 100, 25, 1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "def get_batches(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish epoch 0, loss: 0.05231518510637362, vloss: 3.156096935272217, vAcc: 0.0474\n",
      "Finish epoch 1, loss: 0.051222895122496774, vloss: 3.102509021759033, vAcc: 0.0876\n",
      "Finish epoch 2, loss: 0.05029269986527187, vloss: 3.0528180599212646, vAcc: 0.1282\n",
      "Finish epoch 3, loss: 0.04941140445427487, vloss: 3.0054376125335693, vAcc: 0.2036\n",
      "Finish epoch 4, loss: 0.04849866126933663, vloss: 2.9563992023468018, vAcc: 0.2626\n",
      "Finish epoch 5, loss: 0.04761372110069427, vloss: 2.9091250896453857, vAcc: 0.3046\n",
      "Finish epoch 6, loss: 0.046813398285180434, vloss: 2.8628711700439453, vAcc: 0.3324\n",
      "Finish epoch 7, loss: 0.04594261369559857, vloss: 2.814603328704834, vAcc: 0.35\n",
      "Finish epoch 8, loss: 0.04502811213870121, vloss: 2.764235496520996, vAcc: 0.3634\n",
      "Finish epoch 9, loss: 0.044141344560129005, vloss: 2.716252326965332, vAcc: 0.3628\n",
      "Finish epoch 10, loss: 0.04326634530026077, vloss: 2.669110059738159, vAcc: 0.3678\n",
      "Finish epoch 11, loss: 0.04243877121320503, vloss: 2.6237854957580566, vAcc: 0.3692\n",
      "Finish epoch 12, loss: 0.04163986828793954, vloss: 2.580653190612793, vAcc: 0.3688\n",
      "Finish epoch 13, loss: 0.04085636222767802, vloss: 2.5389912128448486, vAcc: 0.3708\n",
      "Finish epoch 14, loss: 0.04011452044300289, vloss: 2.5000784397125244, vAcc: 0.373\n",
      "Finish epoch 15, loss: 0.03942615513505299, vloss: 2.4643266201019287, vAcc: 0.374\n",
      "Finish epoch 16, loss: 0.03876583685209715, vloss: 2.430325508117676, vAcc: 0.3748\n",
      "Finish epoch 17, loss: 0.038159482224745044, vloss: 2.3972599506378174, vAcc: 0.3774\n",
      "Finish epoch 18, loss: 0.037568060203006653, vloss: 2.36663556098938, vAcc: 0.3786\n",
      "Finish epoch 19, loss: 0.03700554217151852, vloss: 2.337601661682129, vAcc: 0.379\n",
      "Finish epoch 20, loss: 0.036461897080836234, vloss: 2.3088579177856445, vAcc: 0.3804\n",
      "Finish epoch 21, loss: 0.035942656074291375, vloss: 2.2833614349365234, vAcc: 0.384\n",
      "Finish epoch 22, loss: 0.03547845510078063, vloss: 2.2599966526031494, vAcc: 0.392\n",
      "Finish epoch 23, loss: 0.03506297634746377, vloss: 2.2379274368286133, vAcc: 0.3962\n",
      "Finish epoch 24, loss: 0.034677227943751346, vloss: 2.217782735824585, vAcc: 0.399\n",
      "Finish epoch 25, loss: 0.034306249473187174, vloss: 2.198598861694336, vAcc: 0.4014\n",
      "Finish epoch 26, loss: 0.03395540982589632, vloss: 2.1769516468048096, vAcc: 0.4038\n",
      "Finish epoch 27, loss: 0.03353626032088035, vloss: 2.1558432579040527, vAcc: 0.4064\n",
      "Finish epoch 28, loss: 0.0331518053309719, vloss: 2.134784460067749, vAcc: 0.416\n",
      "Finish epoch 29, loss: 0.03275198911306867, vloss: 2.11448073387146, vAcc: 0.4174\n",
      "Finish epoch 30, loss: 0.032352630446412775, vloss: 2.093334674835205, vAcc: 0.4198\n",
      "Finish epoch 31, loss: 0.03192073169654587, vloss: 2.0715115070343018, vAcc: 0.421\n",
      "Finish epoch 32, loss: 0.03153134021222382, vloss: 2.0522923469543457, vAcc: 0.4276\n",
      "Finish epoch 33, loss: 0.03115760399616058, vloss: 2.033714532852173, vAcc: 0.4402\n",
      "Finish epoch 34, loss: 0.030814073150071283, vloss: 2.017909049987793, vAcc: 0.4486\n",
      "Finish epoch 35, loss: 0.030487185625789594, vloss: 2.0004584789276123, vAcc: 0.4562\n",
      "Finish epoch 36, loss: 0.030130285590642503, vloss: 1.9824402332305908, vAcc: 0.465\n",
      "Finish epoch 37, loss: 0.029786485161060184, vloss: 1.967401385307312, vAcc: 0.4708\n",
      "Finish epoch 38, loss: 0.02948554515279611, vloss: 1.9523038864135742, vAcc: 0.4764\n",
      "Finish epoch 39, loss: 0.029191645480264393, vloss: 1.9376591444015503, vAcc: 0.4802\n",
      "Finish epoch 40, loss: 0.028867291118447415, vloss: 1.9225273132324219, vAcc: 0.4848\n",
      "Finish epoch 41, loss: 0.028558103448480962, vloss: 1.9079521894454956, vAcc: 0.4892\n",
      "Finish epoch 42, loss: 0.028294912256641096, vloss: 1.8959741592407227, vAcc: 0.492\n",
      "Finish epoch 43, loss: 0.02805559017454194, vloss: 1.8810949325561523, vAcc: 0.4942\n",
      "Finish epoch 44, loss: 0.02773118620041806, vloss: 1.8664382696151733, vAcc: 0.4966\n",
      "Finish epoch 45, loss: 0.027466515723314544, vloss: 1.8532384634017944, vAcc: 0.4994\n",
      "Finish epoch 46, loss: 0.027183688539413327, vloss: 1.8411318063735962, vAcc: 0.4998\n",
      "Finish epoch 47, loss: 0.02700290604465033, vloss: 1.8313055038452148, vAcc: 0.5024\n",
      "Finish epoch 48, loss: 0.026763767597963214, vloss: 1.8180681467056274, vAcc: 0.5048\n",
      "Finish epoch 49, loss: 0.026461187029499803, vloss: 1.8055492639541626, vAcc: 0.5078\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in get_batches(train_set, train_target, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # Ensure labels are long type for CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    pred = model(valid_set)\n",
    "    vloss =  criterion(pred, valid_target)\n",
    "\n",
    "    pred_index = torch.max(pred, 1)[1]\n",
    "    correct = 0\n",
    "    for j in range (len(valid_target)):\n",
    "        if valid_target[j][pred_index[j]] == 1:\n",
    "            correct +=1\n",
    "    v_acc = correct/len(valid_target)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Finish epoch {}, loss: {}, vloss: {}, vAcc: {}\".format(i, running_loss/len(train_set), vloss, v_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
